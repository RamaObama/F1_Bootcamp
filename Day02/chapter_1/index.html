<!doctype html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Task #1</title>
</head>
<body>
<h2>Поглощение данных: ClickHouse</h2>
<p>
    Тот факт, что ClickHouse не требуется готовить “сегменты”, содержащие все данные и
    попадающие в заданные временные интервалы, позволяет строить более простую
    архитектуру поглощения данных. ClickHouse не требуется ни пакетный движок
    обработки вроде Hadoop, ни “реалтаймовые” узлы. Обычные узлы ClickHouse - те же
    самые, что занимаются хранением данных и обслуживают запросы к ним - напрямую
    принимают пакетные записи данных.
</p>
<p>
    Если таблица разбита на сегменты, то узел, который принимает пакетную запись
    (например, 10к строк) распределяет данные согласно “весам” (смотрите раздел ниже).
    Строки записываются одним пакетом, который формирует небольшое “множество”.
    Множество немедленно конвертируется в колоночный формат. На каждом узле
    ClickHouse работает фоновый процесс, который объединяет наборы строк в еще
    большие наборы. Документация ClickHouse сильно завязана на принцип, известный
    как “MergeTree”, и подчеркивает схожесть его работы с <a
        href="https://ru.wikipedia.org/wiki/LSM-%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE">LSM-деревом</a>, хотя меня это
    слегка смущает, поскольку данные не организованы в деревья - они лежат в плоском
    колончатом формате.
</p>
<h2>Поглощение данных: сравнение</h2>
<p>
    Поглощение данных в Druid и Pinot является “тяжелым”: оно состоит из нескольких
    различных сервисов, и управление ими - это тяжелый труд.
</p>
<p>
    Поглощение данных в ClickHouse гораздо проще (что компенсируется сложностью
    управления “историческими” данными - т.е. данными не в реальном времени), но и
    здесь есть один момент: вы должны иметь возможность собирать данные в пакеты до
    самого ClickHouse. Автоматическое поглощение и пакетный сбор данных из <a
        href="https://clickhouse.com/docs/en/table_engines/kafka.html">Kafka
    доступно “из коробки”</a>, но если у вас используется другой источник данных в реальном
    времени (здесь подразумевается всё, что угодно, в диапазоне между инфраструктурой
    запросов, альтернативной Kafka, и стриминговых движков обработки, вплоть до
    различных HTTP-endpoint), то вам придется создать промежуточный сервис по сбору
    пакетов, или же внести код напрямую в ClickHouse.
</p>
<h2>Выполнение запроса</h2>
<p>
    В <b>Druid и Pinot</b> имеется отдельный слой узлов, называемых <i>“брокерами”</i>, которые
    принимают все запросы к системе. Они определяют, к каким “историческим”
    (<i>содержащим данные не в реальном времени</i>) узлам обработки запросов должны быть
    отправлены подзапросы, основываясь на отображении сегментов в узлы, в которых
    сегменты загружаются. Брокеры хранят информацию об отображении в памяти.
    Брокер-узлы отправляют дальше подзапросы к узлам обработки запросов, и когда
    результаты этих подзапросов возвращаются, брокер объединяет их и возвращает
    финальный комбинированный результат пользователю.
</p>
<p>
    Я не берусь предполагать, зачем при проектировании Druid и Pinot было принято
    решение о введении еще одного типа узлов. Однако, теперь они кажутся их
    неотъемлемой частью, поскольку, когда общее количество сегментов в кластере
    начинает превышать десять миллионов, информация об отображении сегментов в
    узлы начинает занимать гигабайты памяти. Это очень расточительно – выделять
    столько много памяти на каждом узле для обработки запросов. Вот вам и еще один
    недостаток, который накладывается на Druid и Pinot их «сегментированной»
    архитектурой управления данными.
</p>
<p>
    В <b>ClickHouse</b> выделять отдельный набор узлов под “брокер запросов” обычно не
    требуется. Существует специальный, эфемерный <a
        href="https://clickhouse.com/docs/en/table_engines/distributed.html">“распределенный” тип таблицы</a> в
    ClickHouse, который может быть установлен на любом узле, и запросы к этой таблице
    будут делать все то же, за что отвечают брокер-узлы в Druid и Pinot. Обычно подобные
    эфемерные таблицы размещаются на каждом узле, который участвует в
    секционированной таблице, так что на практике каждый узел может быть “входной
    точкой” для запроса в кластер ClickHouse. Этот узел может выпускать необходимые
    подзапросы к другим секциями, обрабатывать свою часть запроса самостоятельно и затем объединять её с частичными
    результатами от других секций.
</p>
<p>
    Когда узел (или один из процессинговых узлов в ClickHouse, или брокер-узел в Druid и
    Pinot) выпускает подзапросы к другим, и один или несколько подзапросов по какой-
    либо причине заканчиваются неудачей, ClickHouse и Pinot обрабатывают эту ситуацию
    правильно: они объединяют результаты успешно выполненных подзапросов вместе, и
    всё равно возвращают частичный результат пользователю. <a
        href="https://leventov.medium.com/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9">Druid
    этой функции сейчас
    очень недостает</a>: если в нем выполнение подзапроса заканчивается неудачей, то
    неудачей закончится и весь запрос целиком.
</p>
<h2>ClickHouse vs. Druid или Pinot: Выводы</h2>
<p>
    “Сегментированный” подход к управлению данными в Druid и Pinot против более
    простого управления данными в ClickHouse определяет многие аспекты систем.
    Однако, важно заметить, что это различие оказывает небольшое (или не оказывает
    вовсе) влияние на потенциальную эффективность сжатия (впрочем, история про
    компрессию для всех трех систем имеет печальный конец по нынешнему состоянию
    дел), или на скорость обработки запросов.
</p>
<p>
    <b>ClickHouse</b> похож на традиционные RDMBS, например, PostgreSQL. В частности,
    ClickHouse можно развернуть на всего один сервер. Если планируемый размер
    невелик - скажем, не больше порядка 100 ядер CPU для обработки запросов и 1 TB
    данных, я бы сказал, что ClickHouse имеет значительные преимущества перед Druid и
    Pinot в силу своей простоты и отсутствия необходимости в дополнительных типах
    узлов, таких как “мастер”, “узлы поглощения в реальном времени”, “брокеры”. На этом
    поле, ClickHouse соревнуется скорее с InfluxDB, чем с Druid или Pinot.
</p>
<p>
    <b>Druid and Pinot</b> похож на системы Big Data вроде HBase. Здесь в виду имеются не
    характеристики производительности, а зависимость от ZooKeper, зависимость от
    персистентного реплицируемого хранилища (к примеру, HDFS), сосредоточение
    внимания на устойчивости к отказам отдельных узлов, а также автономная работа и
    управление данными, не требующими постоянного внимания человека.
</p>
<p>
    Для широкого спектра приложений, ни ClickHouse, ни Druid или Pinot не являются
    очевидными победителями. В первую очередь, вы должны принимать во внимание
    вашу способность разобраться с исходным кодом системы, исправлять баги,
    добавлять новые функции и т.д. Это подробнее обсуждается в разделе “Про
    сравнение производительности и выбор системы”.
</p>
<p>
    Во-вторых, вам стоит взглянуть на таблицу ниже. Каждая ячейка в этой таблице
    описывает свойство приложения, которое позволит определить предпочтительную
    систему. Строки отсортированы не в порядке важности. Важность различных свойств
    может разниться от приложения к приложению, но в целом можно применить
    следующий подход: если ваше приложение соответствует подавляющему
    большинству строк со свойствами в одной из колонок, то относящаяся к ней система в
    вашем случае является предпочтительным выбором.
</p>
<table>
    <thead>
        <tr><th><b>ClickHouse</b></th><th><b>ClickHouse Druid или Pinot</b></th></tr>
    </thead>
    <tbody>
        <tr><td>В организации есть эксперты по C++</td> <td>В организации есть эксперты по Java</td></tr>
        <tr><td>Малый кластер</td><td>Большой кластер</td></tr>
        <tr>
            <td>Немного таблиц</td>
            <td>Много таблиц</td>
        </tr>
        <tr>
            <td>Один набор данных </td>
            <td>Несколько несвязанных наборов данных</td>
        </tr>
        <tr>
            <td>Таблицы и данные находятся в кластере
                перманентно</td>
            <td>Таблицы и наборы данных периодически
                появляются в кластере и удаляются из
                него</td>
        </tr>
        <tr>
            <td>Размер таблиц (и интенсивность
                запросов к ним) остается стабильным во
                времени</td>
            <td>Таблицы значительно растут и
                сжимаются</td>
        </tr>
        <tr>
            <td>Однородные запросы (их тип, размер,
                распределение по времени суток и т.д.)</td>
            <td>Разнородные запросы</td>
        </tr>
        <tr>
            <td>В данных есть измерение, по которому
                оно может быть сегментировано, и почти
                не выполняется запросов, которые
                затрагивают данные, расположенные в
                нескольких сегментах</td>
            <td>Подобного измерения нет, и запросы
                часто затрагивают данные,
                расположенные во всем кластере</td>
        </tr>
        <tr>
            <td>Облако не используется, кластер должен
                быть развернут на специфическую
                конфигурацию физических серверов</td>
            <td>Кластер развернут в облаке</td>
        </tr>
        <tr>
            <td>Нет существующих кластеров Hadoop
                или Spark</td>
            <td>Кластеры Hadoop или Spark уже
                существуют и могут быть использованы</td>
        </tr>
    </tbody>
</table>
</body>
</html>
